---
permalink: /
title: "Lei Cheng"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---


I am a Research Faculty in the Department of Civil and Environmental Engineering (CEE)-[Maryland Transportation Institute (MTI)](https://mti.umd.edu/), [University of Maryland, College Park (UMD)](https://umd.edu/). I am honored to work under the supervision of [Dr. Xianfeng (Terry) Yang](https://cee.umd.edu/clark/faculty/1706/Xianfeng-Terry-Yang) and be part of the [Maryland Transportation & Artificial Intelligence Lab (M-TRAIL)](https://mtrail.umd.edu/). 

Previously, I received my Ph.D. in Electrical and Computer Engineering from the [University of Arizona (UofA)](https://ece.engineering.arizona.edu/) in 2025, where I was fortunate to be advised by [Dr. Siyang Cao](https://ece.engineering.arizona.edu/faculty-staff/faculty/siyang-cao) in the [UA Radar Group](https://github.com/radar-lab). From 2019–2021, I worked at the Artificial Intelligence Research Center, [Peng Cheng Laboratory (PCL)](https://www.pcl.ac.cn/). I earned my M.E. in Integrated Circuit Engineering from [Peking University](https://english.pku.edu.cn/) in 2019 and my B.S. in Applied Physics from [Northeast Petroleum University](https://www.nepu.edu.cn/en/) in 2016. 



My research aims to turn raw sensor data into dependable intelligence for the physical world, with applications in *autonomous driving*, *intelligent transportation systems*, and *smart robotics*. Interests include:
- 📡 **Sensor Data Processing**  
  *Multi-sensor fusion* and *calibration* across Camera · Radar · LiDAR · GNSS, with attention to *space alignment*, *time synchronization*, *uncertainty modeling*, and *long-term stability*.

- 🎯 **Deep Learning-based Perception**  
  Deep models for *object classification*, *detection*, and *multi-object tracking (MOT)*, with the goal of achieving *real-time, reliability-aware perception* in uncertain and dynamic environments. I also seek to develop *perception-driven prediction, control, and planning* strategies that seamlessly translate perception into action, ultimately supporting proactive, safety-critical decisions at system scale.

- 🔀 **Multimodal Learning**  
  *vision–language models* and *Representation learning* that integrate information from *multiple sensors and modalities* (e.g., vision, language, and spatial signals) to couple low-level perception with high-level semantics and reasoning for *comprehensive* environment understanding.


> **Research theme.** *From raw, heterogeneous sensors to reliable, quantitatively validated situational awareness for the real world.*

---

# 🗞️ News
- **2025-03** — 🏆 Our LiDAR–camera calibration tool reached **1k⭐ on GitHub** — preprint & tutorial coming soon.  
- **2025-02** — 📄 One paper on **self-supervised radar fusion** accepted (preprint soon).  
- **2024-12** — 🚀 Released a **dataset mini-benchmark** for bad-weather perception.  
- **2024-10** — 🎙️ Gave a talk on *Reliable Multi-Sensor Fusion in the Wild* (slides & video below).

> Looking for student collaborators and open-source contributors. Ping me on email or GitHub!

---

# 🧪 Selected Projects
**1) CalibWizard** — Plug-and-play **extrinsic calibration** for LiDAR–Camera–Radar  
⭐ *One-click, field-friendly, continuous re-calibration.*  
[Code](#) · [Docs](#) · [Demo](#)

**2) FogFusion** — Robust perception in **fog/rain/snow**  
🌧️ *Adaptive fusion with uncertainty modeling; improves long-tail cases by 18–26%.*  
[Paper](#) · [Code](#) · [Dataset](#)

**3) Radar2Vec** — Self-supervised **radar representation learning**  
🧭 *Contrastive pretraining for radar BEV features; drop-in for modern detectors.*  
[Paper](#) · [Code](#)

**4) DriveLite** — Data-efficient training for **edge deployment**  
⚡ *Latency-aware distillation + INT8 quantization without big accuracy loss.*  
[Paper](#) · [Code](#)

> Want a quick tour? I’m happy to share short Loom demos or live notebooks.

---

# 🧰 My GitHub
<p>
  <a href="https://github.com/leicheng5" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-leicheng5-181717?logo=github" alt="GitHub profile badge">
  </a>
  <a href="https://github.com/leicheng5?tab=repositories" target="_blank">
    <img src="https://img.shields.io/badge/Repositories-Explore-blue" alt="Repositories badge">
  </a>
  <a href="https://scholar.google.com/" target="_blank">
    <img src="https://img.shields.io/badge/Google%20Scholar-Profile-0b6ef6?logo=googlescholar&logoColor=white" alt="Scholar badge">
  </a>
</p>

Featured repos:
- 🔧 **CalibWizard** — Practical LiDAR–Camera–Radar calibration [→](#)  
- 🌫️ **FogFusion** — Weather-robust multi-sensor fusion [→](#)  
- 📡 **Radar2Vec** — Self-supervised radar features [→](#)

---

# 🖼️ Gallery
A few snapshots from projects, talks, and field tests:

<table>
  <tr>
    <td><img src="/images/gallery_01.jpg" alt="Sensor rig" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_02.jpg" alt="Calibration board" style="border-radius:8px;width:100%"></td>
  </tr>
  <tr>
    <td><img src="/images/gallery_03.jpg" alt="Radar BEV" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_04.jpg" alt="Field test" style="border-radius:8px;width:100%"></td>
  </tr>
</table>

> Replace the image paths with your own (e.g., `/images/your_photo.jpg`).  
> Want a video in the gallery? Drop a thumbnail here and link it to YouTube/Vimeo.

