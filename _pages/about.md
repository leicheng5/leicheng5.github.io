---
permalink: /
title: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---


I am a Research Faculty in the Department of Civil and Environmental Engineering (CEE)-[Maryland Transportation Institute (MTI)](https://mti.umd.edu/), [University of Maryland, College Park (UMD)](https://umd.edu/). I am honored to work under the supervision of [Dr. Xianfeng (Terry) Yang](https://cee.umd.edu/clark/faculty/1706/Xianfeng-Terry-Yang) and be part of the [Maryland Transportation & Artificial Intelligence Lab (M-TRAIL)](https://mtrail.umd.edu/). 

Previously, I received my Ph.D. in Electrical and Computer Engineering from the [University of Arizona](https://ece.engineering.arizona.edu/) in 2025, where I was fortunate to be advised by [Dr. Siyang Cao](https://ece.engineering.arizona.edu/faculty-staff/faculty/siyang-cao) in the [UA Radar Group](https://github.com/radar-lab). From 2019–2021, I worked at the Artificial Intelligence Research Center, [Peng Cheng Laboratory (PCL)](https://www.pcl.ac.cn/). I earned my M.E. in Integrated Circuit Engineering from [Peking University](https://english.pku.edu.cn/) in 2019 and my B.S. in Applied Physics from [Northeast Petroleum University](https://www.nepu.edu.cn/en/) in 2016. 



My research aims to turn raw sensor data into dependable intelligence for the physical world, with applications in *autonomous driving*, *intelligent transportation systems*, and *smart robotics*. Interests include:


<div class="smaller-text" markdown="1">
- 📡 **Sensor Data Processing**  
  *Multi-sensor fusion* and *calibration* across Camera · Radar · LiDAR · GNSS, with attention to *space alignment*, *time synchronization*, *uncertainty modeling*, and *long-term stability*.

- 🎯 **Deep Learning-based Perception**  
  Deep models for *object classification*, *detection*, and *multi-object tracking (MOT)*, with the goal of achieving *real-time, reliability-aware perception* in uncertain and dynamic environments. I also seek to develop *perception-driven prediction, control, and planning* strategies that seamlessly translate perception into action, ultimately supporting proactive, safety-critical decisions at system scale.

- 🔀 **Multimodal Learning**  
  *Vision–Language Models (VLMs)* and *Representation Learning* that integrate information from *multiple sensors and modalities* (e.g., vision, language, and spatial signals) to couple low-level perception with high-level semantics and reasoning for *comprehensive* environment understanding. 
</div>



> **Research theme.** *From raw, heterogeneous sensors to reliable, quantitatively validated situational awareness for the real world.*

---


# 🗞️ News  
<div class="smaller-text" markdown="1">
- *[08/2025]* — 🏆 Joined the *University of Maryland, College Park (UMD)* as a Faculty Assistant.  
- *[06/2025]* — 🌟 Featured in the *ECE Class of 2025 Spotlight* at the University of Arizona.  
- *[05/2025]* — 🎓 I defended my *dissertation* and graduated from the University of Arizona!  
- *[01/2025]* — 📄 Paper accepted to *IEEE Transactions on Radar Systems (TRS)*.  
- *[07/2024]* — 📄 Paper accepted to *IEEE Transactions on Intelligent Transportation Systems (T-ITS)*.  
- *[05/2023]* — 🎤 Presented a paper at the *2023 IEEE Radar Conference (RadarConf23)*.  
</div>

> Looking for collaborators and open-source contributors. Ping me on email or GitHub!

---

# 🧪 Selected Projects

<div class="project-entry">
  <div class="project-thumb">
    <img src="/images/calibwizard.png" alt="CalibWizard preview">
  </div>
  <div class="project-info">
    <h3><strong>CalibWizard</strong> — Plug-and-play <em>extrinsic calibration</em> for LiDAR–Camera–Radar</h3>
    <p><em>⭐ One-click, field-friendly, continuous re-calibration.</em></p>
    <p>[<a href="#">Code</a>] · [<a href="#">Docs</a>] · [<a href="#">Demo</a>]</p>
    <p><b>Abstract:</b> A lightweight, real-time calibration framework designed to maintain long-term sensor alignment in dynamic environments.</p>
  </div>
</div>

<div class="project-entry">
  <div class="project-thumb">
    <img src="/images/fogfusion.png" alt="FogFusion preview">
  </div>
  <div class="project-info">
    <h3><strong>FogFusion</strong> — Robust perception in <em>fog/rain/snow</em></h3>
    <p><em>🌧️ Adaptive fusion with uncertainty modeling; improves long-tail cases by 18–26%.</em></p>
    <p>[<a href="#">Paper</a>] · [<a href="#">Code</a>] · [<a href="#">Dataset</a>]</p>
    <p><b>Abstract:</b> A multimodal fusion pipeline that enhances perception reliability under adverse weather.</p>
  </div>
</div>

<div class="project-entry">
  <div class="project-thumb">
    <img src="/images/radar2vec.png" alt="Radar2Vec preview">
  </div>
  <div class="project-info">
    <h3><strong>Radar2Vec</strong> — Self-supervised <em>radar representation learning</em></h3>
    <p><em>🧭 Contrastive pretraining for radar BEV features; drop-in for modern detectors.</em></p>
    <p>[<a href="#">Paper</a>] · [<a href="#">Code</a>]</p>
    <p><b>Abstract:</b> A representation learning framework that unlocks radar's potential with minimal labels.</p>
  </div>
</div>

<div class="project-entry">
  <div class="project-thumb">
    <img src="/images/drivelite.png" alt="DriveLite preview">
  </div>
  <div class="project-info">
    <h3><strong>DriveLite</strong> — Data-efficient training for <em>edge deployment</em></h3>
    <p><em>⚡ Latency-aware distillation + INT8 quantization without big accuracy loss.</em></p>
    <p>[<a href="#">Paper</a>] · [<a href="#">Code</a>]</p>
    <p><b>Abstract:</b> Optimized lightweight models that achieve real-time inference on embedded platforms.</p>
  </div>
</div>


> Want a quick tour? I’m happy to share short Loom demos or live notebooks.

---

# 🧰 My GitHub
<p>
  <a href="https://github.com/leicheng5" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-leicheng5-181717?logo=github" alt="GitHub profile badge">
  </a>
  <a href="https://github.com/leicheng5?tab=repositories" target="_blank">
    <img src="https://img.shields.io/badge/Repositories-Explore-blue" alt="Repositories badge">
  </a>
  <a href="https://scholar.google.com/" target="_blank">
    <img src="https://img.shields.io/badge/Google%20Scholar-Profile-0b6ef6?logo=googlescholar&logoColor=white" alt="Scholar badge">
  </a>
</p>

Featured repos:
- 🔧 **CalibWizard** — Practical LiDAR–Camera–Radar calibration [→](#)  
- 🌫️ **FogFusion** — Weather-robust multi-sensor fusion [→](#)  
- 📡 **Radar2Vec** — Self-supervised radar features [→](#)

---

# 🖼️ Gallery
A few snapshots from projects, talks, and field tests:

<table>
  <tr>
    <td><img src="/images/gallery_01.jpg" alt="Sensor rig" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_02.jpg" alt="Calibration board" style="border-radius:8px;width:100%"></td>
  </tr>
  <tr>
    <td><img src="/images/gallery_03.jpg" alt="Radar BEV" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_04.jpg" alt="Field test" style="border-radius:8px;width:100%"></td>
  </tr>
</table>

> Replace the image paths with your own (e.g., `/images/your_photo.jpg`).  
> Want a video in the gallery? Drop a thumbnail here and link it to YouTube/Vimeo.

