---
permalink: /
title: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---


I am a Research Faculty in the Department of Civil and Environmental Engineering (CEE)-[Maryland Transportation Institute (MTI)](https://mti.umd.edu/), [University of Maryland, College Park (UMD)](https://umd.edu/). I am honored to work under the supervision of [Dr. Xianfeng (Terry) Yang](https://cee.umd.edu/clark/faculty/1706/Xianfeng-Terry-Yang) and be part of the [Maryland Transportation & Artificial Intelligence Lab (M-TRAIL)](https://mtrail.umd.edu/). 

Previously, I received my Ph.D. in Electrical and Computer Engineering from the [University of Arizona](https://ece.engineering.arizona.edu/) in 2025, where I was fortunate to be advised by [Dr. Siyang Cao](https://ece.engineering.arizona.edu/faculty-staff/faculty/siyang-cao) in the [UA Radar Group](https://github.com/radar-lab). From 2019â€“2021, I worked at the Artificial Intelligence Research Center, [Peng Cheng Laboratory (PCL)](https://www.pcl.ac.cn/). I earned my M.E. in Integrated Circuit Engineering from [Peking University](https://english.pku.edu.cn/) in 2019 and my B.S. in Applied Physics from [Northeast Petroleum University](https://www.nepu.edu.cn/en/) in 2016. 



My research aims to turn raw sensor data into dependable intelligence for the physical world, with applications in *autonomous driving*, *intelligent transportation systems*, and *smart robotics*. Interests include:


<small>
- ğŸ“¡ **Sensor Data Processing**  
  *Multi-sensor fusion* and *calibration* across Camera Â· Radar Â· LiDAR Â· GNSS, with attention to *space alignment*, *time synchronization*, *uncertainty modeling*, and *long-term stability*.

- ğŸ¯ **Deep Learning-based Perception**  
  Deep models for *object classification*, *detection*, and *multi-object tracking (MOT)*, with the goal of achieving *real-time, reliability-aware perception* in uncertain and dynamic environments. I also seek to develop *perception-driven prediction, control, and planning* strategies that seamlessly translate perception into action, ultimately supporting proactive, safety-critical decisions at system scale.

- ğŸ”€ **Multimodal Learning**  
  *Visionâ€“Language Models (VLMs)* and *Representation Learning* that integrate information from *multiple sensors and modalities* (e.g., vision, language, and spatial signals) to couple low-level perception with high-level semantics and reasoning for *comprehensive* environment understanding. 
</small>



> **Research theme.** *From raw, heterogeneous sensors to reliable, quantitatively validated situational awareness for the real world.*

---


# ğŸ—ï¸ News  
- *[08/2025]* â€” ğŸ† Joined the *University of Maryland, College Park (UMD)* as a Faculty Assistant.  
- *[06/2025]* â€” ğŸŒŸ Featured in the *ECE Class of 2025 Spotlight* at the University of Arizona.  
- *[05/2025]* â€” ğŸ“ I successfully defended my *Ph.D. dissertation* and graduated from the University of Arizona!  
- *[01/2025]* â€” ğŸ“„ Paper accepted to *IEEE Transactions on Radar Systems (TRS)*.  
- *[07/2024]* â€” ğŸ“„ Paper accepted to *IEEE Transactions on Intelligent Transportation Systems (T-ITS)*.  
- *[05/2023]* â€” ğŸ¤ Presented a paper at the *2023 IEEE Radar Conference (RadarConf23)*.  

> Looking for collaborators and open-source contributors. Ping me on email or GitHub!

---

# ğŸ§ª Selected Projects
**1) CalibWizard** â€” Plug-and-play **extrinsic calibration** for LiDARâ€“Cameraâ€“Radar  
â­ *One-click, field-friendly, continuous re-calibration.*  
[Code](#) Â· [Docs](#) Â· [Demo](#)

**2) FogFusion** â€” Robust perception in **fog/rain/snow**  
ğŸŒ§ï¸ *Adaptive fusion with uncertainty modeling; improves long-tail cases by 18â€“26%.*  
[Paper](#) Â· [Code](#) Â· [Dataset](#)

**3) Radar2Vec** â€” Self-supervised **radar representation learning**  
ğŸ§­ *Contrastive pretraining for radar BEV features; drop-in for modern detectors.*  
[Paper](#) Â· [Code](#)

**4) DriveLite** â€” Data-efficient training for **edge deployment**  
âš¡ *Latency-aware distillation + INT8 quantization without big accuracy loss.*  
[Paper](#) Â· [Code](#)

> Want a quick tour? Iâ€™m happy to share short Loom demos or live notebooks.

---

# ğŸ§° My GitHub
<p>
  <a href="https://github.com/leicheng5" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-leicheng5-181717?logo=github" alt="GitHub profile badge">
  </a>
  <a href="https://github.com/leicheng5?tab=repositories" target="_blank">
    <img src="https://img.shields.io/badge/Repositories-Explore-blue" alt="Repositories badge">
  </a>
  <a href="https://scholar.google.com/" target="_blank">
    <img src="https://img.shields.io/badge/Google%20Scholar-Profile-0b6ef6?logo=googlescholar&logoColor=white" alt="Scholar badge">
  </a>
</p>

Featured repos:
- ğŸ”§ **CalibWizard** â€” Practical LiDARâ€“Cameraâ€“Radar calibration [â†’](#)  
- ğŸŒ«ï¸ **FogFusion** â€” Weather-robust multi-sensor fusion [â†’](#)  
- ğŸ“¡ **Radar2Vec** â€” Self-supervised radar features [â†’](#)

---

# ğŸ–¼ï¸ Gallery
A few snapshots from projects, talks, and field tests:

<table>
  <tr>
    <td><img src="/images/gallery_01.jpg" alt="Sensor rig" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_02.jpg" alt="Calibration board" style="border-radius:8px;width:100%"></td>
  </tr>
  <tr>
    <td><img src="/images/gallery_03.jpg" alt="Radar BEV" style="border-radius:8px;width:100%"></td>
    <td><img src="/images/gallery_04.jpg" alt="Field test" style="border-radius:8px;width:100%"></td>
  </tr>
</table>

> Replace the image paths with your own (e.g., `/images/your_photo.jpg`).  
> Want a video in the gallery? Drop a thumbnail here and link it to YouTube/Vimeo.

